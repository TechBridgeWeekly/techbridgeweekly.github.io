<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <title>Open AI Gym 簡介與 Q learning 演算法實作 | TechBridge 技術共筆部落格</title>
  <meta name="description" content="TechBridge Weekly 技術週刊團隊是一群對用技術改變世界懷抱熱情的團隊。本技術共筆部落格初期專注於Web前後端、行動網路、機器人/物聯網、數據分析與產品設計等技術分享" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- google-site-verification -->
  <meta name="google-site-verification" content="WX_9sZlrIYOEpy8RR7zCoa7-pJk611zZt11BSBUcDVY" />
  <link rel="stylesheet preload" type="text/css" href="/css/screen.css" as="style" />
  <link rel="stylesheet preload" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" as="style" />

  <!-- Favicons -->
  <link rel="apple-touch-icon" href="/img/favicon.ico">
  <link rel="icon preload" href="/img/favicon.ico" as="image">

  
  
  <link rel="alternate" type="application/atom+xml" title="Atom 0.3" href="atom.xml">
  
  

  
</head>


<body class="post-template">

  <header class="site-head" >
    <div class="vertical">
        <div class="site-head-content inner">
             <a class="blog-logo" href="/"><img src="/img/logo-tb-500-500.png" alt="Blog Logo"/></a> 
            <h1 class="blog-title">TechBridge 技術共筆部落格</h1>
            <h2 class="blog-description">var topics = ['Web前後端', '行動網路', '機器人/物聯網', '數據分析', '產品設計', 'etc.']</h2>
            <div class="navbar-block">
                <span><a href="/">首頁</a></span> / <span><a href="/about/">關於我們</a></span> / <span><a href="http://weekly.techbridge.cc/" target="_blank">技術週刊</a></span> / <span><a href="https://www.facebook.com/techbridge.cc/" target="_blank">粉絲專頁</a></span> / <span><a href="/atom.xml" target="_blank">訂閱RSS </a></span>
                <br>
            </div>
        </div>
    </div>
</header>
  

<main class="content" role="main">
  <article class="post">
    <span class="post-meta">
      <time datetime="2017-11-04T22:49:39.000Z" itemprop="datePublished">
          2017-11-04
      </time>
    
    
    | 
    <a href='/tags/Reinforcement-Learning/'>Reinforcement Learning</a>,
    
    <a href='/tags/OpenAI-Gym/'>OpenAI Gym</a>,
    
    <a href='/tags/Q-Learning/'>Q Learning</a>
    
    
</span>

<meta name="generator" content="Open AI Gym 簡介與 Q learning 演算法實作">
<meta name="og:title" content="Open AI Gym 簡介與 Q learning 演算法實作">
<meta name="og:description" content="TechBridge Weekly 技術週刊團隊是一群對用技術改變世界懷抱熱情的團隊。本技術共筆部落格初期專注於Web前後端、行動網路、機器人/物聯網、數據分析與產品設計等技術分享。">
<meta name="og:type" content="website">
<meta name="og:image" content="/img/og-cover.png">

    <h1 class="post-title">Open AI Gym 簡介與 Q learning 演算法實作</h1>
    <section class="post-content">
      <div class="fb-like" data-href="https://www.facebook.com/techbridge.cc" data-layout="button_count" data-action="like" data-size="large" data-show-faces="false" data-share="true"></div>   
      <hr>
      <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>這次我們來跟大家介紹一下 <a target="_blank" rel="noopener" href="https://gym.openai.com/read-only.html">OpenAI Gym</a>，並用裡面的一個環境來實作一個 Q learning 演算法，體會一次 reinforcement learning (以下簡稱 RL) 的概念。</p>
<p>OpenAI Gym 是一個提供許多測試環境的工具，讓大家有一個共同的環境可以測試自己的 RL 演算法，而不用花時間去搭建自己的測試環境。</p>
<h2 id="把-Gym-跑起來的最簡單範例"><a href="#把-Gym-跑起來的最簡單範例" class="headerlink" title="把 Gym 跑起來的最簡單範例"></a>把 Gym 跑起來的最簡單範例</h2><p>一開始學習，範例總是越簡單越好，這樣才會有開始上手的成就感。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">	env.render()</span><br><span class="line">	env.step(env.action_space.sample()) <span class="comment"># take a random action</span></span><br></pre></td></tr></table></figure>

<p>執行這個 .py 檔之後，你應該會看到一個隨便亂動的 cartpole，畫面一下就消失了。</p>
<p>基本上，OpenAI Gym 提供了許許多多的環境，你可以將 CartPole-v0 換成 MountainCar-v0、MsPacman-v0 (需安裝 Atari) 或是 Hopper-v1 (需要安裝 MuJoCo) 等等，你可以在 <a target="_blank" rel="noopener" href="https://gym.openai.com/envs/">這邊</a> 找到更多環境。</p>
<h2 id="Observation"><a href="#Observation" class="headerlink" title="Observation"></a>Observation</h2><p>RL 的一個重要步驟是取得環境狀態，在 Gym 裡面，由 <code>step</code> function 提供環境狀態。<code>step</code> 會回傳 4 個變數，分別是</p>
<ul>
<li>observation (環境狀態)</li>
<li>reward (上一次 action 獲得的 reward )</li>
<li>done (判斷是否達到終止條件的變數)</li>
<li>info ( debug 用的資訊)</li>
</ul>
<p>從呼叫 <code>reset</code>，整個環境就會重頭開始，此外 <code>reset</code> 會回傳一個初始的環境狀態。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">  </span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>): <span class="comment">#how many episodes you want to run</span></span><br><span class="line">	observation = env.reset() <span class="comment">#reset() returns initial observation</span></span><br><span class="line">  </span><br><span class="line">	 <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">		 env.render()</span><br><span class="line">		 <span class="built_in">print</span>(observation)</span><br><span class="line">		 action = env.action_space.sample()</span><br><span class="line">		 observation, reward, done, info = env.step(action)</span><br><span class="line">		 <span class="keyword">if</span> done:</span><br><span class="line">			 <span class="built_in">print</span>(<span class="string">&quot;Episode finished after &#123;&#125; timesteps&quot;</span>.<span class="built_in">format</span>(t+<span class="number">1</span>))</span><br><span class="line">			 <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>執行上面這一段程式碼，你就會看到每一步收到的環境狀態不斷地被印在 terminal。</p>
<h2 id="Space"><a href="#Space" class="headerlink" title="Space"></a>Space</h2><p>除了 observation 之外，RL 中另一個重點就是要定義可以做的 action，這兩者都由 space 來定義。</p>
<p>大家可以使用下面的程式碼來查看 action space 跟 observation space。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line"><span class="comment">## Check dimension of spaces ##</span></span><br><span class="line"><span class="built_in">print</span>(env.action_space)</span><br><span class="line"><span class="comment">#&gt; Discrete(2)</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space)</span><br><span class="line"><span class="comment">#&gt; Box(4,)</span></span><br><span class="line"><span class="comment">## Check range of spaces ##</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">print(env.action_space.high)-</span></span><br><span class="line"><span class="string">You&#x27;ll get error if you run this, because &#x27;Discrete&#x27; object has no attribute &#x27;high&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space.high)</span><br><span class="line"><span class="built_in">print</span>(env.observation_space.low)</span><br></pre></td></tr></table></figure>

<p>此外，你也可以 assign 自己的 action space，像下例中就把 action space 設成只包含一個 action，所以 agent 就永遠只能採取同一種 action，你看得出來是向左還是向右嗎？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line">  </span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">env.action_space = spaces.Discrete(<span class="number">1</span>) <span class="comment"># Set it to only 1 elements &#123;0&#125;</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>): <span class="comment">#how many episodes you want to run</span></span><br><span class="line">	observation = env.reset() <span class="comment">#reset() returns initial observation</span></span><br><span class="line"></span><br><span class="line"> 	<span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">		 env.render()</span><br><span class="line">		 <span class="built_in">print</span>(observation)</span><br><span class="line">		 action = env.action_space.sample()</span><br><span class="line">		 observation, reward, done, info = env.step(action)</span><br><span class="line">		 <span class="keyword">if</span> done:</span><br><span class="line">			 <span class="built_in">print</span>(<span class="string">&quot;Episode finished after &#123;&#125; timesteps&quot;</span>.<span class="built_in">format</span>(t+<span class="number">1</span>))</span><br><span class="line">			 <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h2 id="來學習一個真正有學習能力的演算法-Q-learning"><a href="#來學習一個真正有學習能力的演算法-Q-learning" class="headerlink" title="來學習一個真正有學習能力的演算法 - Q-learning"></a>來學習一個真正有學習能力的演算法 - Q-learning</h2><p>經過上面的介紹，大家應該對 Gym 有了基本的認識，也跟 RL 最重要的 observation 和 action 銜接起來了。接下來就是今天的重頭戲，讓我們來真正實作一個演算法來學著不讓 pole 倒下來。</p>
<p>關於 Q leanring，推薦大家直接看這一小段 <a target="_blank" rel="noopener" href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-2-A-q-learning/">Q learning 演算法介紹</a>，看完應該就可以直接懂這個演算法：</p>
<p><img src="https://i.stack.imgur.com/JvJqR.png" alt="Q learning algorithm"></p>
<p>裡面只有一處比較不直覺，就是在更新 Q table 時，計算 reward 不只包含採取 action $a$ 獲得的 reward $r$，還包含 $\gamma max_{a’}Q(s’, a’)$。這個概念是，agent 不僅僅看當下採取的行動帶來的好處，他也會估計到達下一個 state $s’$ 後，最多可以有多少好處（因為在 $s’$ 也可以採取各種 action）。換句話說，這個 agent 不是一個目光如豆的 agent，他會考慮未來。因為加上了 $\gamma max_{a’}Q(s’, a’)$ (當然，$\gamma$ 不能是 0)，讓我們的 agent 從 <a target="_blank" rel="noopener" href="http://smartold.businessweekly.com.tw/webarticle.php?id=48582">會立刻吃掉棉花糖的小朋友，進化成可以晚一點再吃多一點棉花糖的小朋友</a>，是不是很有趣呢！</p>
<p>經過以上的說明，大家應該可以了解 Q learning 演算法的核心概念了。這時大家可能會有點疑惑，之前好像有聽過 Deep Q learning，那跟 Q learning 差在哪邊呢？其實就只是有沒有使用到 Deep neural network 而已，如果你理解這個演算法，應該不難發現他的能力滿有限的，很難拿來學習完成複雜的 task，所以才有人引入 DNN 來讓其學習能力變得更強。</p>
<h2 id="實作-Q-learning"><a href="#實作-Q-learning" class="headerlink" title="實作 Q learning"></a>實作 Q learning</h2><p>程式碼是參考 <a target="_blank" rel="noopener" href="https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947">這篇文章</a> ，裡面有介紹詳細的思考及調整過程。有興趣深入了解的讀者可以參考看看。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment">## Initialize the &quot;Cart-Pole&quot; environment</span></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">  </span><br><span class="line"><span class="comment">## Defining the environment related constants</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Number of discrete states (bucket) per state dimension</span></span><br><span class="line">NUM_BUCKETS = (<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>) <span class="comment"># (x, x&#x27;, theta, theta&#x27;)</span></span><br><span class="line"><span class="comment"># Number of discrete actions</span></span><br><span class="line">NUM_ACTIONS = env.action_space.n <span class="comment"># (left, right)</span></span><br><span class="line"><span class="comment"># Bounds for each discrete state</span></span><br><span class="line">STATE_BOUNDS = <span class="built_in">list</span>(<span class="built_in">zip</span>(env.observation_space.low, env.observation_space.high))</span><br><span class="line">STATE_BOUNDS[<span class="number">1</span>] = [-<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">STATE_BOUNDS[<span class="number">3</span>] = [-math.radians(<span class="number">50</span>), math.radians(<span class="number">50</span>)]</span><br><span class="line"><span class="comment"># Index of the action</span></span><br><span class="line">ACTION_INDEX = <span class="built_in">len</span>(NUM_BUCKETS)</span><br><span class="line">  </span><br><span class="line"><span class="comment">## Creating a Q-Table for each state-action pair</span></span><br><span class="line">q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))</span><br><span class="line">  </span><br><span class="line"><span class="comment">## Learning related constants</span></span><br><span class="line">MIN_EXPLORE_RATE = <span class="number">0.01</span></span><br><span class="line">MIN_LEARNING_RATE = <span class="number">0.1</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">## Defining the simulation related constants</span></span><br><span class="line">NUM_EPISODES = <span class="number">1000</span></span><br><span class="line">MAX_T = <span class="number">250</span></span><br><span class="line">STREAK_TO_END = <span class="number">120</span></span><br><span class="line">SOLVED_T = <span class="number">199</span></span><br><span class="line">DEBUG_MODE = <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simulate</span>():</span><br><span class="line">  </span><br><span class="line"><span class="comment">## Instantiating the learning related parameters</span></span><br><span class="line">	learning_rate = get_learning_rate(<span class="number">0</span>)</span><br><span class="line">explore_rate = get_explore_rate(<span class="number">0</span>)</span><br><span class="line">discount_factor = <span class="number">0.99</span> <span class="comment"># since the world is unchanging</span></span><br><span class="line">  </span><br><span class="line">num_streaks = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(NUM_EPISODES):</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Reset the environment</span></span><br><span class="line">	obv = env.reset()</span><br><span class="line">  </span><br><span class="line"><span class="comment"># the initial state</span></span><br><span class="line">state_0 = state_to_bucket(obv)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(MAX_T):</span><br><span class="line">	env.render()</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Select an action</span></span><br><span class="line">	action = select_action(state_0, explore_rate)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Execute the action</span></span><br><span class="line">obv, reward, done, _ = env.step(action)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Observe the result</span></span><br><span class="line">state = state_to_bucket(obv)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Update the Q based on the result</span></span><br><span class="line">best_q = np.amax(q_table[state])</span><br><span class="line">q_table[state_0 + (action,)] += learning_rate*(reward + discount_factor*(best_q) - q_table[state_0 + (action,)])</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Setting up for the next iteration</span></span><br><span class="line">state_0 = state</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Print data</span></span><br><span class="line"><span class="keyword">if</span> (DEBUG_MODE):</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;\nEpisode = %d&quot;</span> % episode)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;t = %d&quot;</span> % t)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Action: %d&quot;</span> % action)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;State: %s&quot;</span> % <span class="built_in">str</span>(state))</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Reward: %f&quot;</span> % reward)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Best Q: %f&quot;</span> % best_q)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Explore rate: %f&quot;</span> % explore_rate)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Learning rate: %f&quot;</span> % learning_rate)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Streaks: %d&quot;</span> % num_streaks)</span><br><span class="line">  </span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  </span><br><span class="line">	<span class="keyword">if</span> done:</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&quot;Episode %d finished after %f time steps&quot;</span> % (episode, t))</span><br><span class="line">	<span class="keyword">if</span> (t &gt;= SOLVED_T):</span><br><span class="line">		num_streaks += <span class="number">1</span></span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">		num_streaks = <span class="number">0</span></span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#sleep(0.25)</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># It&#x27;s considered done when it&#x27;s solved over 120 times consecutively</span></span><br><span class="line">		<span class="keyword">if</span> num_streaks &gt; STREAK_TO_END:</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Update parameters</span></span><br><span class="line">		explore_rate = get_explore_rate(episode)</span><br><span class="line">learning_rate = get_learning_rate(episode)</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">select_action</span>(<span class="params">state, explore_rate</span>):</span><br><span class="line"><span class="comment"># Select a random action</span></span><br><span class="line">	<span class="keyword">if</span> random.random() &lt; explore_rate:</span><br><span class="line">	action = env.action_space.sample()</span><br><span class="line"><span class="comment"># Select the action with the highest q</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">action = np.argmax(q_table[state])</span><br><span class="line"><span class="keyword">return</span> action</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_explore_rate</span>(<span class="params">t</span>):</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">max</span>(MIN_EXPLORE_RATE, <span class="built_in">min</span>(<span class="number">1</span>, <span class="number">1.0</span> - math.log10((t+<span class="number">1</span>)/<span class="number">25</span>)))</span><br><span class="line">  </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">get_learning_rate</span>(<span class="params">t</span>):</span><br><span class="line">		<span class="keyword">return</span> <span class="built_in">max</span>(MIN_LEARNING_RATE, <span class="built_in">min</span>(<span class="number">0.5</span>, <span class="number">1.0</span> - math.log10((t+<span class="number">1</span>)/<span class="number">25</span>)))</span><br><span class="line">  </span><br><span class="line">		<span class="keyword">def</span> <span class="title function_">state_to_bucket</span>(<span class="params">state</span>):</span><br><span class="line">			bucket_indice = []</span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(state)):</span><br><span class="line">				<span class="keyword">if</span> state[i] &lt;= STATE_BOUNDS[i][<span class="number">0</span>]:</span><br><span class="line">				bucket_index = <span class="number">0</span></span><br><span class="line">				<span class="keyword">elif</span> state[i] &gt;= STATE_BOUNDS[i][<span class="number">1</span>]:</span><br><span class="line">				bucket_index = NUM_BUCKETS[i] - <span class="number">1</span></span><br><span class="line">				<span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># Mapping the state bounds to the bucket array</span></span><br><span class="line">				bound_width = STATE_BOUNDS[i][<span class="number">1</span>] - STATE_BOUNDS[i][<span class="number">0</span>]</span><br><span class="line">				offset = (NUM_BUCKETS[i]-<span class="number">1</span>)*STATE_BOUNDS[i][<span class="number">0</span>]/bound_width</span><br><span class="line">				scaling = (NUM_BUCKETS[i]-<span class="number">1</span>)/bound_width</span><br><span class="line">				bucket_index = <span class="built_in">int</span>(<span class="built_in">round</span>(scaling*state[i] - offset))</span><br><span class="line">bucket_indice.append(bucket_index)</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">tuple</span>(bucket_indice)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">simulate()</span><br></pre></td></tr></table></figure>

<h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>這篇文章跟大家說明了 OpenAI Gym 裡面的基本組成，也介紹了 Q learning 演算法及實作。有興趣更深入研究的讀者可以以此為基礎，繼續鑽研。</p>
<h2 id="延伸閱讀"><a href="#延伸閱讀" class="headerlink" title="延伸閱讀"></a>延伸閱讀</h2><ol>
<li><a target="_blank" rel="noopener" href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/1-1-A-RL/">什麼是強化學習</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/rll/rllab">rllab - a framework for developing and evaluating reinforcement learning algorithms</a></li>
</ol>
<p>關於作者：<br><a target="_blank" rel="noopener" href="https://pojenlai.wordpress.com/">@pojenlai</a> 演算法工程師，對機器人跟電腦視覺有少許研究，最近在學習<a target="_blank" rel="noopener" href="https://buzzorange.com/techorange/2017/07/10/elon-musk-first-principle/">看清事物的本質與改進自己的觀念</a></p>
  
      <div>喜歡我們的文章嗎？歡迎分享按讚給予我們支持和鼓勵！</div>
      <div class="fb-like" data-href="https://blog.techbridge.cc/2017/11/04/openai-gym-intro-and-q-learning/index.html" data-layout="button_count" data-action="like" data-size="large" data-show-faces="false" data-share="true"></div>
      <br>
      <br>
      <div class="fb-page" data-href="https://www.facebook.com/techbridge.cc" data-small-header="false" data-adapt-container-width="true" data-hide-cover="false" data-show-facepile="true"><blockquote cite="https://www.facebook.com/techbridge.cc" class="fb-xfbml-parse-ignore"><a target="_blank" rel="noopener" href="https://www.facebook.com/techbridge.cc">TechBridge 技術日報</a></blockquote></div>
      <br>
    </section>
    <br>
    <hr>
    <div>
      <h4>訂閱 TechBridge Weekly 技術週刊，每週發送最精華的技術開發、產品設計的資訊給您</h4>
      <form class="form-control" method="post" action="https://goodbits.io/e/cab8a418-6b70-48d6-97ea-b5f0ef34b22c" target="_blank">
        <input class="form-control" type="text" name="first_name" placeholder="First Name"></input>
        <input class="form-control" type="text" name="last_name" placeholder="Last Name"></input>
        <div>
          <input class="form-control" type="text" name="email" placeholder="Email"></input>
        </div>
        <br>
        <div>
          <button class="form-control btn subscribe-btn" type="submit">馬上訂閱技術週刊</button>
        </div>
        <br>
        <label for="">PS. 我們討厭垃圾信，所以我們只提供有價值的內容給您 :)</label>
      </form>
    </div>
    <footer class="post-footer">
      <section class="author">
    <h4>TechBridge Weekly 技術週刊編輯團隊</h4>
    <p>TechBridge Weekly 技術週刊團隊是一群對用技術改變世界懷抱熱情的團隊。本技術共筆部落格初期專注於Web前後端、行動網路、機器人/物聯網、資料科學與產品設計等技術分享。This is TechBridge Weekly Team Tech Blog, which focus on web, mobile, robotics, IoT, Data Science technology sharing.</p>
    <span><a href="/2016/03/19/about/">關於我們</a></span> / <span><a href="https://www.techbridge.cc/" target="_blank">技術日報</a></span> / <span><a href="http://weekly.techbridge.cc/" target="_blank">技術週刊</a></span> / <span><a href="https://www.facebook.com/techbridge.cc/" target="_blank">粉絲專頁</a></span> / <span><a href="/atom.xml" target="_blank">訂閱RSS </a></span>   
	<div class="fb-like" data-href="https://www.facebook.com/techbridge.cc" data-layout="button_count" data-size="large" data-action="like" data-show-faces="false" data-share="true"></div>    
</section>
      <section class="share">
    <h4>Share this post</h4>
    <a class="icon-twitter" target="_blank" rel="noopener" href="http://twitter.com/share?url=https://blog.techbridge.cc/2017/11/04/openai-gym-intro-and-q-learning/"
       onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
        <span class="hidden">Twitter</span>
    </a>
    <a class="icon-facebook" target="_blank" rel="noopener" href="https://www.facebook.com/sharer/sharer.php?u=https://blog.techbridge.cc/2017/11/04/openai-gym-intro-and-q-learning/"
       onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
        <span class="hidden">Facebook</span>
    </a>
    <a class="icon-google-plus" target="_blank" rel="noopener" href="https://plus.google.com/share?url=https://blog.techbridge.cc/2017/11/04/openai-gym-intro-and-q-learning/"
       onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
        <span class="hidden">Google+</span>
    </a>
    <iframe src="https://ghbtns.com/github-btn.html?user=TechBridgeHQ&repo=blog-starter-kit&type=star&count=true" frameborder="0" scrolling="0" width="170px" height="20px"></iframe>      
</section>
    </footer>
    <br>
  </article>
  <nav class="pagination" role="pagination">
    <h2>更多優質技術文章</h2>
    
    <a class="newer-posts" href="/2017/11/11/cs50/">
        ← 別猶豫了，來修 CS50 吧！
    </a>
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/2017/10/28/python-data-science-and-machine-learning-pandas-tutorial/">
        用 Python 自學資料科學與機器學習入門實戰：Pandas 基礎入門 →
    </a>
    
</nav>
  <div id="comment" class="comments-area">
    <h1 class="title"><a href="#disqus_comments" name="disqus_comments">留言討論</a></h1>

    
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>
    
</div>
</main>


  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75308642-1', 'auto');
  ga('send', 'pageview');

</script>
<footer class="site-footer">
  
  <a class="subscribe icon-feed" href="/atom.xml"><span class="tooltip">Subscribe!</span></a>
  
  <div class="inner">
     <section class="copyright">All content copyright <a href="/">TechBridge 技術共筆部落格</a> &copy; 2017 &bull; All rights reserved.</section>
  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script type="text/javascript" src="/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/js/index.js"></script>


<script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', '[object Object]']);
    _gaq.push(['_trackPageview']);

    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
</script>


<script type="text/javascript">
    var disqus_shortname = 'techbridgeweekly';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.4/dist/medium-zoom.min.js"></script>
<script>
// NodeList
mediumZoom(document.querySelectorAll('img'));
</script>
  <div id="fb-root"></div>
	<script>(function(d, s, id) {
	  var js, fjs = d.getElementsByTagName(s)[0];
	  if (d.getElementById(id)) return;
	  js = d.createElement(s); js.id = id;
	  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5";
	  fjs.parentNode.insertBefore(js, fjs);
	}(document, 'script', 'facebook-jssdk'));</script>
</body>
</html>
