<!DOCTYPE html>
<html>

<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>用 Python 自學資料科學與機器學習入門實戰：Scikit Learn 基礎入門 | TechBridge 技術共筆部落格</title>
  <meta name="description" content="TechBridge Weekly 技術週刊團隊是一群對用技術改變世界懷抱熱情的團隊。本技術共筆部落格初期專注於Web前後端、行動網路、機器人/物聯網、數據分析與產品設計等技術分享">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- google-site-verification -->
  <meta name="google-site-verification" content="WX_9sZlrIYOEpy8RR7zCoa7-pJk611zZt11BSBUcDVY">
  <link rel="stylesheet preload" type="text/css" href="/css/screen.css" as="style">
  <link rel="stylesheet preload" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" as="style">

  <!-- Favicons -->
  <link rel="apple-touch-icon" href="/img/favicon.ico">
  <link rel="icon preload" href="/img/favicon.ico" as="image">

  
  
  <link rel="alternate" type="application/atom+xml" title="Atom 0.3" href="atom.xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  

  
</head>


<body class="post-template">

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="site-head" >
    <div class="vertical">
        <div class="site-head-content inner">
             <a class="blog-logo" href="/"><img src="/img/logo-tb-500-500.png" alt="Blog Logo"/></a> 
            <h1 class="blog-title">TechBridge 技術共筆部落格</h1>
            <h2 class="blog-description">var topics = ['Web前後端', '行動網路', '機器人/物聯網', '數據分析', '產品設計', 'etc.']</h2>
            <div class="navbar-block">
                <span><a href="/">首頁</a></span> / <span><a href="/about/">關於我們</a></span> / <span><a href="http://weekly.techbridge.cc/" target="_blank">技術週刊</a></span> / <span><a href="https://www.facebook.com/techbridge.cc/" target="_blank">粉絲專頁</a></span> / <span><a href="/atom.xml" target="_blank">訂閱RSS </a></span>
                <br>
            </div>
        </div>
    </div>
</header>
  

<main class="content" role="main">
  <article class="post">
    <span class="post-meta">
      <time datetime="2017-11-24T09:54:49.000Z" itemprop="datePublished">
          2017-11-24
      </time>
    
    
    | 
    <a href='/tags/Python-Machine-Learning-機器學習-AI-Artificial-Intelligence-NLP-Data-Mining-人工智慧-監督式學習-Supervised-learning-從零開始學資料科學-Numpy-資料科學-data-science-data-scientist-pandas/'>Python, Machine Learning, 機器學習, AI, Artificial Intelligence, NLP, Data Mining, 人工智慧, 監督式學習, Supervised learning, 從零開始學資料科學, Numpy, 資料科學, data science, data scientist, pandas</a>
    
    
</span>

<meta name="generator" content="用 Python 自學資料科學與機器學習入門實戰：Scikit Learn 基礎入門">
<meta name="og:title" content="用 Python 自學資料科學與機器學習入門實戰：Scikit Learn 基礎入門">
<meta name="og:description" content="TechBridge Weekly 技術週刊團隊是一群對用技術改變世界懷抱熱情的團隊。本技術共筆部落格初期專注於Web前後端、行動網路、機器人/物聯網、數據分析與產品設計等技術分享。">
<meta name="og:type" content="website">
<meta name="og:image" content="/img/og-cover.png">

    <h1 class="post-title">用 Python 自學資料科學與機器學習入門實戰：Scikit Learn 基礎入門</h1>
    <section class="post-content">
      <div class="fb-like" data-href="https://www.facebook.com/techbridge.cc" data-layout="button_count" data-action="like" data-size="large" data-show-faces="false" data-share="true"></div>   
      <hr>
      <p><img src="/img/kdchang/data-science101/scikit-learn/scikit-learn-logo.png" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本系列文章將透過 Python 及其資料科學與機器學習生態系（Numpy、Scipy、Pandas、scikit-learn、Statsmodels、Matplotlib、Scrapy、Keras、TensorFlow 等）來系統性介紹資料科學與機器學習相關的知識。在這個單元中我們將介紹 scikit-learn 這個機器學習和資料分析神兵利器和基本的機器學習工作流程。接下來我們的範例將會使用 Ananconda、Python3 和 Jupyter Notebook 開發環境進行，若還沒安裝環境的讀者記得先行安裝。首先我們先來認識一下基本機器學習工作流程，讓讀者對於機器學習工作流有基本而全面的認識。</p>
<h1 id="基本機器學習工作流程（Machine-Learning-Workflow）"><a href="#基本機器學習工作流程（Machine-Learning-Workflow）" class="headerlink" title="基本機器學習工作流程（Machine Learning Workflow）"></a>基本機器學習工作流程（Machine Learning Workflow）</h1><p><img src="/img/kdchang/data-science101/scikit-learn/ml-workflow1.jpg" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<ol>
<li>明確定義問題 (Problem Definition)</li>
<li>獲取資料與探索性資料分析 (Get Data &amp; Exploratory Data Analysis)</li>
<li>資料預處理與特徵工程 (Data Clean/Preprocessing &amp; Feature Engineering)</li>
<li>訓練模型與校調 (Model Training)</li>
<li>模型驗證 (Model Predict &amp; Testing)</li>
<li>模型優化 (Model Optimization)</li>
<li>上線運行 (Deploy Model)</li>
</ol>
<h1 id="明確定義問題-Problem-Definition"><a href="#明確定義問題-Problem-Definition" class="headerlink" title="明確定義問題 (Problem Definition)"></a>明確定義問題 (Problem Definition)</h1><p><img src="/img/kdchang/data-science101/scikit-learn/iris.jpg" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<p>明確定義問題是進行機器學習工作流的第一步。由於機器學習和一般的 Web 網頁應用程式開發比較不一樣，其需要的運算資源和時間成本比較高，若能一開始就定義好問題並將問題抽象為數學問題將有助於我們要蒐集的資料集和節省工作流程的時間。</p>
<p>舉例來說，本篇文章範例希望預測 Iris 鳶尾花屬於哪一個類別（setosa 山鳶尾、versicolor 變色鳶尾、virginica 維吉尼亞鳶尾），這邊我們就可以決定是要進行有對應結果的監督式學習：二元分類問題（binary classification）、多類別分類問題（multi-classification）還是連續量的迴歸問題（regression），或是沒有標籤結果的非監督式學習（例如：clustering）等，我們這邊假設這是一個多類別分類問題：給定未知資料希望能預測花朵屬於哪一類。換句話說，就是說我們先定義好我們想要解決或是預測的問題，然後去蒐集對應的資料。</p>
<h1 id="獲取資料與探索性資料分析-Get-Data-amp-Exploratory-Data-Analysis"><a href="#獲取資料與探索性資料分析-Get-Data-amp-Exploratory-Data-Analysis" class="headerlink" title="獲取資料與探索性資料分析 (Get Data &amp; Exploratory Data Analysis)"></a>獲取資料與探索性資料分析 (Get Data &amp; Exploratory Data Analysis)</h1><p><img src="/img/kdchang/data-science101/scikit-learn/irises.png" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<p>基本上資料集的完整性某種程度決定了預測結果是否能發揮模型最大功效。由於我們是教學文章，這邊我們的範例使用 scikit-learn 內建的玩具資料集 Iris（鳶尾花）的花萼、花蕊長寬進行花朵類別判別（setosa 山鳶尾、versicolor 變色鳶尾、virginica 維吉尼亞鳶尾）。在這個資料集中已經幫我們標註好每筆資料對應的類別，所以我們可以視為多類別分類問題（multi-classification）。</p>
<p><img src="/img/kdchang/data-science101/scikit-learn/iris-feature.gif" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<p>引入模組</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入 numpy、pd 和 sklearn(scikit-learn) 模組</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="comment"># 引入 train_test_split 分割方法，注意在 sklearn v0.18 後 train_test_split 從 sklearn.cross_validation 子模組搬到 sklearn.model_selection 中</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 引入 KNeighbors 模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br></pre></td></tr></table></figure>
<p>引入資料集並進行探索性資料分析</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入 iris 資料集</span></span><br><span class="line">raw_iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 探索性分析 Exploratory data analysis，了解資料集內容</span></span><br><span class="line"><span class="comment"># 先印出 key 值，列出有哪些值：['data', 'target', 'target_names', 'DESCR', 'feature_names']</span></span><br><span class="line">print(raw_iris.keys())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 印出 feature 值</span></span><br><span class="line">print(raw_iris[<span class="string">'data'</span>])</span><br><span class="line"><span class="comment"># 印出目標值，分別對應的是三種花的類別：['setosa 山鳶尾' 'versicolor 變色鳶尾' 'virginica 維吉尼亞鳶尾']</span></span><br><span class="line">print(raw_iris[<span class="string">'target'</span>])</span><br><span class="line"><span class="comment"># 印出目標標籤，三種花的類別：['setosa' 'versicolor' 'virginica']</span></span><br><span class="line">print(raw_iris[<span class="string">'target_names'</span>])</span><br><span class="line"><span class="comment"># 印出資料集內容描述</span></span><br><span class="line">print(raw_iris[<span class="string">'DESCR'</span>])</span><br><span class="line"><span class="comment"># 印出屬性名稱，['sepal length 花萼長度 (cm)', 'sepal width 花萼寬度 (cm)', 'petal length 花蕊長度 (cm)', 'petal width 花蕊寬度 (cm)']</span></span><br><span class="line">print(raw_iris[<span class="string">'feature_names'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 類別種類</span></span><br><span class="line">print(np.unique(raw_iris.target))</span><br></pre></td></tr></table></figure>
<pre><code>dict_keys([&apos;data&apos;, &apos;target&apos;, &apos;target_names&apos;, &apos;DESCR&apos;, &apos;feature_names&apos;])
[[ 5.1  3.5  1.4  0.2]
 [ 4.9  3.   1.4  0.2]
 [ 4.7  3.2  1.3  0.2]
 [ 4.6  3.1  1.5  0.2]
 [ 5.   3.6  1.4  0.2]
 [ 5.4  3.9  1.7  0.4]
 [ 4.6  3.4  1.4  0.3]
 [ 5.   3.4  1.5  0.2]
 [ 4.4  2.9  1.4  0.2]
 [ 4.9  3.1  1.5  0.1]
 [ 5.4  3.7  1.5  0.2]
 [ 4.8  3.4  1.6  0.2]
 [ 4.8  3.   1.4  0.1]
 [ 4.3  3.   1.1  0.1]
 [ 5.8  4.   1.2  0.2]
 [ 5.7  4.4  1.5  0.4]
 [ 5.4  3.9  1.3  0.4]
 [ 5.1  3.5  1.4  0.3]
 [ 5.7  3.8  1.7  0.3]
 [ 5.1  3.8  1.5  0.3]
 [ 5.4  3.4  1.7  0.2]
 [ 5.1  3.7  1.5  0.4]
 [ 4.6  3.6  1.   0.2]
 [ 5.1  3.3  1.7  0.5]
 [ 4.8  3.4  1.9  0.2]
 [ 5.   3.   1.6  0.2]
 [ 5.   3.4  1.6  0.4]
 [ 5.2  3.5  1.5  0.2]
 [ 5.2  3.4  1.4  0.2]
 [ 4.7  3.2  1.6  0.2]
 [ 4.8  3.1  1.6  0.2]
 [ 5.4  3.4  1.5  0.4]
 [ 5.2  4.1  1.5  0.1]
 [ 5.5  4.2  1.4  0.2]
 [ 4.9  3.1  1.5  0.1]
 [ 5.   3.2  1.2  0.2]
 [ 5.5  3.5  1.3  0.2]
 [ 4.9  3.1  1.5  0.1]
 [ 4.4  3.   1.3  0.2]
 [ 5.1  3.4  1.5  0.2]
 [ 5.   3.5  1.3  0.3]
 [ 4.5  2.3  1.3  0.3]
 [ 4.4  3.2  1.3  0.2]
 [ 5.   3.5  1.6  0.6]
 [ 5.1  3.8  1.9  0.4]
 [ 4.8  3.   1.4  0.3]
 [ 5.1  3.8  1.6  0.2]
 [ 4.6  3.2  1.4  0.2]
 [ 5.3  3.7  1.5  0.2]
 [ 5.   3.3  1.4  0.2]
 [ 7.   3.2  4.7  1.4]
 [ 6.4  3.2  4.5  1.5]
 [ 6.9  3.1  4.9  1.5]
 [ 5.5  2.3  4.   1.3]
 [ 6.5  2.8  4.6  1.5]
 [ 5.7  2.8  4.5  1.3]
 [ 6.3  3.3  4.7  1.6]
 [ 4.9  2.4  3.3  1. ]
 [ 6.6  2.9  4.6  1.3]
 [ 5.2  2.7  3.9  1.4]
 [ 5.   2.   3.5  1. ]
 [ 5.9  3.   4.2  1.5]
 [ 6.   2.2  4.   1. ]
 [ 6.1  2.9  4.7  1.4]
 [ 5.6  2.9  3.6  1.3]
 [ 6.7  3.1  4.4  1.4]
 [ 5.6  3.   4.5  1.5]
 [ 5.8  2.7  4.1  1. ]
 [ 6.2  2.2  4.5  1.5]
 [ 5.6  2.5  3.9  1.1]
 [ 5.9  3.2  4.8  1.8]
 [ 6.1  2.8  4.   1.3]
 [ 6.3  2.5  4.9  1.5]
 [ 6.1  2.8  4.7  1.2]
 [ 6.4  2.9  4.3  1.3]
 [ 6.6  3.   4.4  1.4]
 [ 6.8  2.8  4.8  1.4]
 [ 6.7  3.   5.   1.7]
 [ 6.   2.9  4.5  1.5]
 [ 5.7  2.6  3.5  1. ]
 [ 5.5  2.4  3.8  1.1]
 [ 5.5  2.4  3.7  1. ]
 [ 5.8  2.7  3.9  1.2]
 [ 6.   2.7  5.1  1.6]
 [ 5.4  3.   4.5  1.5]
 [ 6.   3.4  4.5  1.6]
 [ 6.7  3.1  4.7  1.5]
 [ 6.3  2.3  4.4  1.3]
 [ 5.6  3.   4.1  1.3]
 [ 5.5  2.5  4.   1.3]
 [ 5.5  2.6  4.4  1.2]
 [ 6.1  3.   4.6  1.4]
 [ 5.8  2.6  4.   1.2]
 [ 5.   2.3  3.3  1. ]
 [ 5.6  2.7  4.2  1.3]
 [ 5.7  3.   4.2  1.2]
 [ 5.7  2.9  4.2  1.3]
 [ 6.2  2.9  4.3  1.3]
 [ 5.1  2.5  3.   1.1]
 [ 5.7  2.8  4.1  1.3]
 [ 6.3  3.3  6.   2.5]
 [ 5.8  2.7  5.1  1.9]
 [ 7.1  3.   5.9  2.1]
 [ 6.3  2.9  5.6  1.8]
 [ 6.5  3.   5.8  2.2]
 [ 7.6  3.   6.6  2.1]
 [ 4.9  2.5  4.5  1.7]
 [ 7.3  2.9  6.3  1.8]
 [ 6.7  2.5  5.8  1.8]
 [ 7.2  3.6  6.1  2.5]
 [ 6.5  3.2  5.1  2. ]
 [ 6.4  2.7  5.3  1.9]
 [ 6.8  3.   5.5  2.1]
 [ 5.7  2.5  5.   2. ]
 [ 5.8  2.8  5.1  2.4]
 [ 6.4  3.2  5.3  2.3]
 [ 6.5  3.   5.5  1.8]
 [ 7.7  3.8  6.7  2.2]
 [ 7.7  2.6  6.9  2.3]
 [ 6.   2.2  5.   1.5]
 [ 6.9  3.2  5.7  2.3]
 [ 5.6  2.8  4.9  2. ]
 [ 7.7  2.8  6.7  2. ]
 [ 6.3  2.7  4.9  1.8]
 [ 6.7  3.3  5.7  2.1]
 [ 7.2  3.2  6.   1.8]
 [ 6.2  2.8  4.8  1.8]
 [ 6.1  3.   4.9  1.8]
 [ 6.4  2.8  5.6  2.1]
 [ 7.2  3.   5.8  1.6]
 [ 7.4  2.8  6.1  1.9]
 [ 7.9  3.8  6.4  2. ]
 [ 6.4  2.8  5.6  2.2]
 [ 6.3  2.8  5.1  1.5]
 [ 6.1  2.6  5.6  1.4]
 [ 7.7  3.   6.1  2.3]
 [ 6.3  3.4  5.6  2.4]
 [ 6.4  3.1  5.5  1.8]
 [ 6.   3.   4.8  1.8]
 [ 6.9  3.1  5.4  2.1]
 [ 6.7  3.1  5.6  2.4]
 [ 6.9  3.1  5.1  2.3]
 [ 5.8  2.7  5.1  1.9]
 [ 6.8  3.2  5.9  2.3]
 [ 6.7  3.3  5.7  2.5]
 [ 6.7  3.   5.2  2.3]
 [ 6.3  2.5  5.   1.9]
 [ 6.5  3.   5.2  2. ]
 [ 6.2  3.4  5.4  2.3]
 [ 5.9  3.   5.1  1.8]]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
[&apos;setosa&apos; &apos;versicolor&apos; &apos;virginica&apos;]
Iris Plants Database
====================

Notes
-----
Data Set Characteristics:
    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

This is a copy of UCI ML iris datasets.
http://archive.ics.uci.edu/ml/datasets/Iris

The famous Iris database, first used by Sir R.A Fisher

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher&apos;s paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

References
----------
   - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot;
     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to
     Mathematical Statistics&quot; (John Wiley, NY, 1950).
   - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...

[&apos;sepal length (cm)&apos;, &apos;sepal width (cm)&apos;, &apos;petal length (cm)&apos;, &apos;petal width (cm)&apos;]
[0 1 2]
</code></pre><h1 id="資料預處理與特徵工程-Data-Clean-Preprocessing-amp-Feature-Engineering"><a href="#資料預處理與特徵工程-Data-Clean-Preprocessing-amp-Feature-Engineering" class="headerlink" title="資料預處理與特徵工程 (Data Clean/Preprocessing &amp; Feature Engineering)"></a>資料預處理與特徵工程 (Data Clean/Preprocessing &amp; Feature Engineering)</h1><p><img src="/img/kdchang/data-science101/scikit-learn/ml-pipeline.png" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<p>良好的資料輸入取決於資料預處理與特徵工程，而好的輸入將大大影響到模型是否可以發揮其理論正常水準。以下把資料轉成 DataFrame 格式方便進行操作。由於這邊的資料集已經是相當完整的資料集，所以我們這邊就不用特別進行資料預處理和特徵工程的部份，然而在真實世界中，真正在進行機器學習工作流程的時候資料預處理往往是最花時間的部份。同時為了方便模型的校調，我們這邊把資料集分為 70% 訓練資料，30% 驗證資料。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將資料轉為 pandas DataFrame</span></span><br><span class="line"><span class="comment"># data 為觀察目標變數</span></span><br><span class="line">df_X = pd.DataFrame(raw_iris.data)</span><br><span class="line"><span class="comment"># target 為預測變數</span></span><br><span class="line">df_y = pd.DataFrame(raw_iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將資料切分為 training data 和 testing data，其中 random_state 若設為 0 或不設則即便實例不同但因種子相同產生同樣隨機編號，若設為 1 則每次隨機產生不同編號</span></span><br><span class="line"><span class="comment"># test_size 為切分 training data 和 testing data 的比例</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 印出所有資料集筆數</span></span><br><span class="line">print(len(df_y))</span><br></pre></td></tr></table></figure>
<pre><code>150
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 印出切分 y_train 的數量為所有資料集的 70%，共 105 筆</span></span><br><span class="line">print(y_train)</span><br><span class="line">print(len(y_train))</span><br></pre></td></tr></table></figure>
<pre><code>     0
39   0
106  2
99   1
0    0
16   0
118  2
80   1
29   0
11   0
104  2
100  2
72   1
108  2
42   0
20   0
31   0
115  2
111  2
89   1
83   1
130  2
41   0
66   1
120  2
113  2
6    0
126  2
62   1
23   0
97   1
..  ..
10   0
76   1
129  2
144  2
137  2
12   0
79   1
123  2
127  2
36   0
74   1
37   0
131  2
110  2
22   0
32   0
147  2
134  2
102  2
75   1
88   1
148  2
33   0
56   1
28   0
90   1
82   1
25   0
121  2
13   0

[105 rows x 1 columns]
105
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 印出切分的 y_test 資料為所有資料集的 30%，共 45 筆</span></span><br><span class="line">print(y_test)</span><br><span class="line">print(len(y_test))</span><br></pre></td></tr></table></figure>
<pre><code>     0
102  2
53   1
143  2
70   1
61   1
67   1
24   0
124  2
36   0
92   1
114  2
31   0
120  2
87   1
74   1
47   0
69   1
56   1
93   1
16   0
144  2
133  2
29   0
57   1
116  2
50   1
86   1
44   0
3    0
21   0
82   1
99   1
134  2
111  2
135  2
4    0
2    0
91   1
85   1
122  2
127  2
11   0
27   0
79   1
51   1
45
</code></pre><h1 id="訓練模型與校調-Model-Training"><a href="#訓練模型與校調-Model-Training" class="headerlink" title="訓練模型與校調 (Model Training)"></a>訓練模型與校調 (Model Training)</h1><p><img src="/img/kdchang/data-science101/scikit-learn/scikit-learn-map.png" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<p>上面是 scikit-learn 提供的演算法 cheat-sheet，當你面對琳琅滿目的模型一開始不知道要選擇什麼的話可以按圖索驥參考，另外這邊提供<a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" target="_blank" rel="noopener">大圖支援連結</a>。</p>
<p>這邊我們參考上圖來選擇適合模型：</p>
<ol>
<li>樣本資料是否大於 50 筆：範例資料集總共有 150 筆資料，大於 50</li>
<li>是否為分類問題：Iris 花朵類別預測是多類別分類問題</li>
<li>是否有標籤好的資料：已經有 label 資料</li>
<li>樣本資料是否小於 100K：資料小於 100K</li>
<li>選擇 Linear SVC 模型（第一個選擇的模型）</li>
<li>是否是文字資料：不是</li>
<li>選擇 KNeighborsClassifier 模型（第二個選擇的模型）</li>
<li>後續優化 / SVC / Ensemble</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化 LinearSVC 實例</span></span><br><span class="line">lin_clf = LinearSVC()</span><br><span class="line"><span class="comment"># 使用 fit 來建置模型，其參數接收 training data matrix, testing data array，所以進行 y_train.values.ravel() Data Frame 轉換</span></span><br><span class="line">lin_clf.fit(X_train, y_train.values.ravel())</span><br></pre></td></tr></table></figure>
<pre><code>LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss=&apos;squared_hinge&apos;, max_iter=1000,
     multi_class=&apos;ovr&apos;, penalty=&apos;l2&apos;, random_state=None, tol=0.0001,
     verbose=0)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化 KNeighborsClassifier 實例</span></span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 fit 來建置模型，其參數接收 training data matrix, testing data array，所以進行 y_train.values.ravel() 轉換</span></span><br><span class="line">knn.fit(X_train, y_train.values.ravel())</span><br></pre></td></tr></table></figure>
<pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights=&apos;uniform&apos;)
</code></pre><h1 id="模型驗證-Model-Predict-amp-Testing"><a href="#模型驗證-Model-Predict-amp-Testing" class="headerlink" title="模型驗證 (Model Predict &amp; Testing)"></a>模型驗證 (Model Predict &amp; Testing)</h1><p><img src="/img/kdchang/data-science101/scikit-learn/train-test-phase.png" alt=" 如何使用 Python 學習機器學習（Machine Learning）"></p>
<p>監督式學習的分類問題通常會分為訓練模型和驗證模型，這邊我們使用 predict 去產生對應的目標值，此時和正確答案（已經標籤好的目標值）比較可以知道模型預測的正確率。我們可以看到 KNeighborsClassifier 在正確率（accuracy）表現上相對比較好一點（0.98 比 0.93）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 X_test 來預測結果</span></span><br><span class="line">print(lin_clf.predict(X_test))</span><br></pre></td></tr></table></figure>
<pre><code>[1 1 0 2 1 2 0 1 1 2 2 1 0 2 0 0 2 1 2 0 0 1 2 0 2 1 2 0 0 0 1 0 2 1 1 0 0
 0 1 0 1 2 2 1 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 印出預測準確率</span></span><br><span class="line">print(lin_clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<pre><code>0.933333333333
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 X_test 來預測結果</span></span><br><span class="line">print(knn.predict(X_test))</span><br></pre></td></tr></table></figure>
<pre><code>[1 1 0 1 1 2 0 1 1 2 2 1 0 2 0 0 2 1 2 0 0 1 2 0 2 1 2 0 0 0 1 0 1 1 1 0 0
 0 1 0 1 2 2 1 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 印出 testing data 預測標籤機率</span></span><br><span class="line">print(knn.predict_proba(X_test))</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.   1.   0. ]
 [ 0.   1.   0. ]
 [ 1.   0.   0. ]
 [ 0.   0.8  0.2]
 [ 0.   1.   0. ]
 [ 0.   0.   1. ]
 [ 1.   0.   0. ]
 [ 0.   1.   0. ]
 [ 0.   1.   0. ]
 [ 0.   0.   1. ]
 [ 0.   0.   1. ]
 [ 0.   1.   0. ]
 [ 1.   0.   0. ]
 [ 0.   0.   1. ]
 [ 1.   0.   0. ]
 [ 1.   0.   0. ]
 [ 0.   0.   1. ]
 [ 0.   1.   0. ]
 [ 0.   0.   1. ]
 [ 1.   0.   0. ]
 [ 1.   0.   0. ]
 [ 0.   1.   0. ]
 [ 0.   0.   1. ]
 [ 1.   0.   0. ]
 [ 0.   0.2  0.8]
 [ 0.   1.   0. ]
 [ 0.   0.   1. ]
 [ 1.   0.   0. ]
 [ 1.   0.   0. ]
 [ 1.   0.   0. ]
 [ 0.   1.   0. ]
 [ 1.   0.   0. ]
 [ 0.   1.   0. ]
 [ 0.   1.   0. ]
 [ 0.   1.   0. ]
 [ 1.   0.   0. ]
 [ 1.   0.   0. ]
 [ 1.   0.   0. ]
 [ 0.   0.6  0.4]
 [ 1.   0.   0. ]
 [ 0.   1.   0. ]
 [ 0.   0.   1. ]
 [ 0.   0.   1. ]
 [ 0.   1.   0. ]
 [ 0.   1.   0. ]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 印出預測準確率</span></span><br><span class="line">print(knn.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<pre><code>0.977777777778
</code></pre><h1 id="模型優化-Model-Optimization"><a href="#模型優化-Model-Optimization" class="headerlink" title="模型優化 (Model Optimization)"></a>模型優化 (Model Optimization)</h1><p>由於本文是簡易範例，這邊就沒有示範如何進行模型優化（這邊可以嘗試使用 SVC 和 Ensemble 方法）。不過一般來說在分類模型優化上，讓模型預測表現的更好的方法大約有幾種：</p>
<ol>
<li>特徵工程：選擇更適合特徵值或是更好的資料清理，某種程度上很需要專業知識的協助（domain konwledge）去發現和整合出更好的 feature</li>
<li>調整模型參數：調整模型的參數</li>
<li>模型融合：結合幾個弱分類器結果來變成強的分類器</li>
</ol>
<h1 id="上線運行-Deploy-Model"><a href="#上線運行-Deploy-Model" class="headerlink" title="上線運行 (Deploy Model)"></a>上線運行 (Deploy Model)</h1><p>當模型優化完畢就可以進行上線運行，其中 Python 比 R 更具優勢的地方，那就是 Python 很容易跟現有的系統進行整合，Python 也有許多好用的 Web 框架可以使用，也因為 Python 是膠水語言，若要進行效能優化也可以很容易呼叫 C/C++ 進行操作，提昇執行效能。</p>
<h1 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h1><p>以上用一個簡單的範例介紹了 Python 機器學習套件 Scikit Learn 的基本功能和機器學習整個基本 Workflow。由於是基礎範例所以省略一些比較繁瑣的資料處理部分，事實上，真實世界資料大多是非結構化資料的髒資料，而資料分析的過程往往需要花上許多時間進行資料預處理和資料清理上。接下來我們將介紹其他 Python 資料科學和機器學習生態系和相關工具。</p>
<h1 id="延伸閱讀"><a href="#延伸閱讀" class="headerlink" title="延伸閱讀"></a>延伸閱讀</h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/23191325" target="_blank" rel="noopener">机器学习实战 之 kNN 分类</a></li>
<li><a href="http://www.chengweihuang.com/machine-learning-workflow.html" target="_blank" rel="noopener">Machine Learning Workflow</a></li>
<li><a href="https://www.leiphone.com/news/201709/zYIOJqMzR0mJARzj.html" target="_blank" rel="noopener">Kaggle机器学习之模型融合（stacking）心得</a></li>
<li><a href="https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations" target="_blank" rel="noopener">Python 資料視覺化</a></li>
<li><a href="http://insidebigdata.com/wp-content/uploads/2016/06/MathWorks1.jpg" target="_blank" rel="noopener">Train &amp; Predict workflow</a></li>
<li><a href="http://www.nd.com/wp-content/uploads/2016/12/classification-pipeline-1.png" target="_blank" rel="noopener">Sample Classification Pipeline workflow</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25836678" target="_blank" rel="noopener">【机器学习】模型融合方法概述</a></li>
</ol>
<p>關於作者：<br><a href="http://blog.kdchang.cc" target="_blank" rel="noopener">@kdchang</a> 文藝型開發者，夢想是做出人們想用的產品和辦一所心目中理想的學校。A Starter, Software Engineer &amp; Maker. JavaScript, Python &amp; Arduino/Android lover. Interested in Internet, AI and Blockchain.:)</p>
<p>（image via <a href="https://cdn-images-1.medium.com/max/2000/1*KzmIUYPmxgEHhXX7SlbP4w.jpeg" target="_blank" rel="noopener">medium</a>、<a href="https://mapr.com/blog/predicting-breast-cancer-using-apache-spark-machine-learning-logistic-regression/assets/blogimages/bcmlprocess.png" target="_blank" rel="noopener">mapr</a>、<a href="https://silvrback.s3.amazonaws.com/uploads/408cfbbb-9e9b-4590-9501-9b2775f0d9c9/ml_pipeline_large.png" target="_blank" rel="noopener">silvrback</a>、<a href="https://camo.githubusercontent.com/a560481c1c29e98437016be493a236cb826c757d/687474703a2f2f312e62702e626c6f6773706f742e636f6d2f2d4d45323465507a707a494d2f55514c57547775726658492f4141414141414141414e772f573345455449726f4138302f73313630302f64726f705f736861646f77735f6261636b67726f756e642e706e67" target="_blank" rel="noopener">camo</a>、<a href="http://www.scipy-lectures.org/_images/scikit-learn-logo.png" target="_blank" rel="noopener">scipy</a>、<a href="http://mirlab.org/jang/books/dcpr/image/iris.gif" target="_blank" rel="noopener">mirlab</a>、<a href="http://articles.concreteinteractive.com/wp-content/uploads/2015/03/irises.png" target="_blank" rel="noopener">concreteinteractive</a>、<a href="http://hgtvhome.sndimg.com/content/dam/img/kdchang/grdn/fullset/2014/2/5/0/12-waltersgardens-hi14643-irisautumn-circus.jpg.rend.hgtvcom.1280.853.suffix/1452644697576.jpeg" target="_blank" rel="noopener">sndimg</a>）</p>
  
      <div>喜歡我們的文章嗎？歡迎分享按讚給予我們支持和鼓勵！</div>
      <div class="fb-like" data-href="https://blog.techbridge.cc/2017/11/24/python-data-science-and-machine-learning-scikit-learn-basic-tutorial/index.html" data-layout="button_count" data-action="like" data-size="large" data-show-faces="false" data-share="true"></div>
      <br>
      <br>
      <div class="fb-page" data-href="https://www.facebook.com/techbridge.cc" data-small-header="false" data-adapt-container-width="true" data-hide-cover="false" data-show-facepile="true"><blockquote cite="https://www.facebook.com/techbridge.cc" class="fb-xfbml-parse-ignore"><a href="https://www.facebook.com/techbridge.cc">TechBridge 技術日報</a></blockquote></div>
      <br>
    </section>
    <br>
    <hr>
    <div>
      <h4>訂閱 TechBridge Weekly 技術週刊，每週發送最精華的技術開發、產品設計的資訊給您</h4>
      <form class="form-control" method="post" action="https://goodbits.io/e/cab8a418-6b70-48d6-97ea-b5f0ef34b22c" target="_blank">
        <input class="form-control" type="text" name="first_name" placeholder="First Name"></input>
        <input class="form-control" type="text" name="last_name" placeholder="Last Name"></input>
        <div>
          <input class="form-control" type="text" name="email" placeholder="Email"></input>
        </div>
        <br>
        <div>
          <button class="form-control btn subscribe-btn" type="submit">馬上訂閱技術週刊</button>
        </div>
        <br>
        <label for="">PS. 我們討厭垃圾信，所以我們只提供有價值的內容給您 :)</label>
      </form>
    </div>
    <footer class="post-footer">
      <section class="author">
    <h4>TechBridge Weekly 技術週刊編輯團隊</h4>
    <p>TechBridge Weekly 技術週刊團隊是一群對用技術改變世界懷抱熱情的團隊。本技術共筆部落格初期專注於Web前後端、行動網路、機器人/物聯網、資料科學與產品設計等技術分享。This is TechBridge Weekly Team Tech Blog, which focus on web, mobile, robotics, IoT, Data Science technology sharing.</p>
    <span><a href="/2016/03/19/about/">關於我們</a></span> / <span><a href="https://www.techbridge.cc/" target="_blank">技術日報</a></span> / <span><a href="http://weekly.techbridge.cc/" target="_blank">技術週刊</a></span> / <span><a href="https://www.facebook.com/techbridge.cc/" target="_blank">粉絲專頁</a></span> / <span><a href="/atom.xml" target="_blank">訂閱RSS </a></span>   
	<div class="fb-like" data-href="https://www.facebook.com/techbridge.cc" data-layout="button_count" data-size="large" data-action="like" data-show-faces="false" data-share="true"></div>    
</section>
      <section class="share">
    <h4>Share this post</h4>
    <a class="icon-twitter" href="http://twitter.com/share?url=https://blog.techbridge.cc/2017/11/24/python-data-science-and-machine-learning-scikit-learn-basic-tutorial/"
       onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
        <span class="hidden">Twitter</span>
    </a>
    <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://blog.techbridge.cc/2017/11/24/python-data-science-and-machine-learning-scikit-learn-basic-tutorial/"
       onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
        <span class="hidden">Facebook</span>
    </a>
    <a class="icon-google-plus" href="https://plus.google.com/share?url=https://blog.techbridge.cc/2017/11/24/python-data-science-and-machine-learning-scikit-learn-basic-tutorial/"
       onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
        <span class="hidden">Google+</span>
    </a>
    <iframe src="https://ghbtns.com/github-btn.html?user=TechBridgeHQ&repo=blog-starter-kit&type=star&count=true" frameborder="0" scrolling="0" width="170px" height="20px"></iframe>      
</section>
    </footer>
    <br>
  </article>
  <nav class="pagination" role="pagination">
    <h2>更多優質技術文章</h2>
    
    <a class="newer-posts" href="/2017/12/01/pycon-2017-huan-le-xue-python-wei-yuan-zu-ma-byte-code/">
        ← 歡樂學 Python 位元組碼(byte code)
    </a>
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/2017/11/17/drawing-css-image/">
        用 CSS 畫畫的小技巧 →
    </a>
    
</nav>
  <div id="comment" class="comments-area">
    <h1 class="title"><a href="#disqus_comments" name="disqus_comments">留言討論</a></h1>

    
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>
    
</div>
</main>


  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75308642-1', 'auto');
  ga('send', 'pageview');

</script>
<footer class="site-footer">
  
  <a class="subscribe icon-feed" href="/atom.xml"><span class="tooltip">Subscribe!</span></a>
  
  <div class="inner">
     <section class="copyright">All content copyright <a href="/">TechBridge 技術共筆部落格</a> &copy; 2017 &bull; All rights reserved.</section>
  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script type="text/javascript" src="/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/js/index.js"></script>


<script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', '[object Object]']);
    _gaq.push(['_trackPageview']);

    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
</script>


<script type="text/javascript">
    var disqus_shortname = 'techbridgeweekly';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.4/dist/medium-zoom.min.js"></script>
<script>
// NodeList
mediumZoom(document.querySelectorAll('img'));
</script>
  <div id="fb-root"></div>
	<script>(function(d, s, id) {
	  var js, fjs = d.getElementsByTagName(s)[0];
	  if (d.getElementById(id)) return;
	  js = d.createElement(s); js.id = id;
	  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5";
	  fjs.parentNode.insertBefore(js, fjs);
	}(document, 'script', 'facebook-jssdk'));</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
